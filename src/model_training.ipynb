{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://huggingface.co/d4data/biomedical-ner-all\n",
    "https://huggingface.co/datasets/singh-aditya/MACCROBAT_biomedical_ner\n",
    "\n",
    "https://www.freecodecamp.org/news/getting-started-with-ner-models-using-huggingface/\n",
    "https://medium.com/@minhle_0210/pos-tagging-medical-ner-ffcdaef7a7b3\n",
    "https://github.com/dreji18/Bio-Epidemiology-NER\n",
    "\n",
    "https://huggingface.co/distilbert/distilbert-base-uncased\n",
    "https://huggingface.co/google-bert/bert-base-uncased\n",
    "https://huggingface.co/emilyalsentzer/Bio_ClinicalBERT\n",
    "https://huggingface.co/Charangan/MedBERT\n",
    "\n",
    "https://wandb.ai/jack-morris/david-vs-goliath/reports/Does-Model-Size-Matter-A-Comparison-of-BERT-and-DistilBERT--VmlldzoxMDUxNzU#:~:text=The%20BERT%20authors%20recommend%20fine,5e%2D5%2C%203e%2D5\n",
    "https://datascience.stackexchange.com/questions/64583/what-are-the-good-parameter-ranges-for-bert-hyperparameters-while-finetuning-it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity_group': 'Sign_symptom', 'score': 0.9999311, 'word': 'palpitations', 'start': 38, 'end': 50}, {'entity_group': 'Clinical_event', 'score': 0.99975544, 'word': 'follow', 'start': 54, 'end': 60}, {'entity_group': 'Date', 'score': 0.999867, 'word': '6 months after', 'start': 64, 'end': 78}]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"d4data/biomedical-ner-all\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"d4data/biomedical-ner-all\")\n",
    "\n",
    "pipe = pipeline(\"ner\", model=model, tokenizer=tokenizer, device=0, aggregation_strategy=\"max\")  # device=0 (gpu)\n",
    "\n",
    "text = \"\"\"The patient reported no recurrence of palpitations at follow-up 6 months after the ablation.\"\"\"\n",
    "\n",
    "out = pipe(text)\n",
    "print(out)\n",
    "\n",
    "spans = []\n",
    "\n",
    "for row in out:\n",
    "    spans.append((row[\"start\"], row[\"end\"], row[\"entity_group\"]))\n",
    "\n",
    "# show_span_ascii_markup(text, spans)\n",
    "show_span_box_markup(text, spans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import gc\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import pipeline, AutoTokenizer, DataCollatorForTokenClassification, AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "from transformers.utils.logging import set_verbosity_error\n",
    "\n",
    "from ipymarkup import show_span_ascii_markup, show_span_box_markup\n",
    "\n",
    "from mrb_ner_config import *\n",
    "from mrb_ner_dataset import MRBNERDataset\n",
    "from mrb_ner_evaluator import MRBNEREvaluator\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get dataset from huggingface\n",
    "data = load_dataset(DATA_PATH)\n",
    "\n",
    "# extract NER labels\n",
    "label_names = data[\"train\"].features[\"ner_labels\"].feature.names\n",
    "id2label = {}\n",
    "label2id = {}\n",
    "\n",
    "for idx, label in enumerate(label_names):\n",
    "    id2label[idx] = label\n",
    "    label2id[label] = idx\n",
    "\n",
    "# split dataset into train, val and test splits\n",
    "data_splits = {}\n",
    "data_splits[\"train\"], data_splits[\"val\"], data_splits[\"test\"]  = random_split(data[\"train\"], [0.7, 0.15, 0.15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------------- Training Model (Model:distilbert-base-uncased, Lr:5e-05) ----------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 35/35 [00:05<00:00,  7.30it/s]\n",
      "100%|██████████| 35/35 [00:05<00:00,  7.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.166745662689209, 'eval_overall_precision': 0.031503579952267304, 'eval_overall_recall': 0.0234208658623137, 'eval_overall_f1': 0.026867494402605333, 'eval_overall_accuracy': 0.1462021062065875, 'eval_runtime': 0.5138, 'eval_samples_per_second': 58.383, 'eval_steps_per_second': 15.569, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35/35 [00:06<00:00,  5.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 6.4521, 'train_samples_per_second': 21.698, 'train_steps_per_second': 5.425, 'train_loss': 2.5291458129882813, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:00<00:00, 19.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating best accuracy: None -> 0.1610647693817468\n",
      "\n",
      "---------------------- Training Model (Model:distilbert-base-uncased, Lr:4e-05) ----------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 35/35 [00:04<00:00,  7.29it/s]\n",
      "100%|██████████| 35/35 [00:05<00:00,  7.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.2539150714874268, 'eval_overall_precision': 0.02373887240356083, 'eval_overall_recall': 0.014194464158977998, 'eval_overall_f1': 0.01776593382189651, 'eval_overall_accuracy': 0.10015684517140937, 'eval_runtime': 0.5125, 'eval_samples_per_second': 58.542, 'eval_steps_per_second': 15.611, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35/35 [00:06<00:00,  5.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 6.2015, 'train_samples_per_second': 22.575, 'train_steps_per_second': 5.644, 'train_loss': 2.6125560215541292, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:00<00:00, 18.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------------- Training Model (Model:distilbert-base-uncased, Lr:3e-05) ----------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 35/35 [00:05<00:00,  7.23it/s]\n",
      "100%|██████████| 35/35 [00:05<00:00,  7.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.4129912853240967, 'eval_overall_precision': 0.0, 'eval_overall_recall': 0.0, 'eval_overall_f1': 0.0, 'eval_overall_accuracy': 0.01646874299798342, 'eval_runtime': 0.509, 'eval_samples_per_second': 58.939, 'eval_steps_per_second': 15.717, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35/35 [00:06<00:00,  5.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 6.4458, 'train_samples_per_second': 21.72, 'train_steps_per_second': 5.43, 'train_loss': 2.7533719744001117, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:00<00:00, 18.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------------- Training Model (Model:distilbert-base-uncased, Lr:2e-05) ----------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 35/35 [00:04<00:00,  7.22it/s]\n",
      "100%|██████████| 35/35 [00:05<00:00,  7.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.574371099472046, 'eval_overall_precision': 0.0, 'eval_overall_recall': 0.0, 'eval_overall_f1': 0.0, 'eval_overall_accuracy': 0.0, 'eval_runtime': 0.5068, 'eval_samples_per_second': 59.193, 'eval_steps_per_second': 15.785, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35/35 [00:06<00:00,  5.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 6.394, 'train_samples_per_second': 21.895, 'train_steps_per_second': 5.474, 'train_loss': 2.914011710030692, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:00<00:00, 18.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving best distilbert-base-uncased model...\n",
      "test_loss: 1.9678986072540283\n",
      "test_overall_precision: 0.050677666470241606\n",
      "test_overall_recall: 0.03194650817236255\n",
      "test_overall_f1: 0.039188881294144454\n",
      "test_overall_accuracy: 0.1610647693817468\n",
      "test_runtime: 0.464\n",
      "test_samples_per_second: 64.655\n",
      "test_steps_per_second: 17.241\n",
      "\n",
      "---------------------- Training Model (Model:biobert, Lr:5e-05) ----------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at dmis-lab/biobert-v1.1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 35/35 [00:09<00:00,  3.60it/s]\n",
      "100%|██████████| 35/35 [00:10<00:00,  3.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.3059959411621094, 'eval_overall_precision': 0.08184143222506395, 'eval_overall_recall': 0.011895910780669145, 'eval_overall_f1': 0.020772476468679, 'eval_overall_accuracy': 0.11390887290167866, 'eval_runtime': 0.867, 'eval_samples_per_second': 34.601, 'eval_steps_per_second': 9.227, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35/35 [00:11<00:00,  2.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 11.9027, 'train_samples_per_second': 11.762, 'train_steps_per_second': 2.941, 'train_loss': 2.7616962977818083, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:00<00:00, 11.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating best accuracy: None -> 0.09998827804477788\n",
      "\n",
      "---------------------- Training Model (Model:biobert, Lr:4e-05) ----------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at dmis-lab/biobert-v1.1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 35/35 [00:09<00:00,  3.58it/s]\n",
      "100%|██████████| 35/35 [00:10<00:00,  3.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.415416955947876, 'eval_overall_precision': 0.0, 'eval_overall_recall': 0.0, 'eval_overall_f1': 0.0, 'eval_overall_accuracy': 0.017876607804665358, 'eval_runtime': 0.869, 'eval_samples_per_second': 34.521, 'eval_steps_per_second': 9.206, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35/35 [00:11<00:00,  2.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 11.8709, 'train_samples_per_second': 11.793, 'train_steps_per_second': 2.948, 'train_loss': 2.84061519077846, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:00<00:00, 11.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------------- Training Model (Model:biobert, Lr:3e-05) ----------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at dmis-lab/biobert-v1.1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 35/35 [00:09<00:00,  3.61it/s]\n",
      "100%|██████████| 35/35 [00:10<00:00,  3.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.5255775451660156, 'eval_overall_precision': 0.0, 'eval_overall_recall': 0.0, 'eval_overall_f1': 0.0, 'eval_overall_accuracy': 0.00021800741225201656, 'eval_runtime': 0.8947, 'eval_samples_per_second': 33.532, 'eval_steps_per_second': 8.942, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35/35 [00:12<00:00,  2.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 12.4834, 'train_samples_per_second': 11.215, 'train_steps_per_second': 2.804, 'train_loss': 2.946702575683594, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:00<00:00, 10.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------------- Training Model (Model:biobert, Lr:2e-05) ----------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at dmis-lab/biobert-v1.1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 35/35 [00:09<00:00,  3.89it/s]\n",
      "100%|██████████| 35/35 [00:10<00:00,  3.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.7253005504608154, 'eval_overall_precision': 0.0, 'eval_overall_recall': 0.0, 'eval_overall_f1': 0.0, 'eval_overall_accuracy': 0.0, 'eval_runtime': 0.833, 'eval_samples_per_second': 36.014, 'eval_steps_per_second': 9.604, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35/35 [00:11<00:00,  3.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 11.6692, 'train_samples_per_second': 11.997, 'train_steps_per_second': 2.999, 'train_loss': 3.1398714338030134, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:00<00:00, 11.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving best biobert model...\n",
      "test_loss: 2.1792454719543457\n",
      "test_overall_precision: 0.08142493638676845\n",
      "test_overall_recall: 0.012307692307692308\n",
      "test_overall_f1: 0.021383227530905442\n",
      "test_overall_accuracy: 0.09998827804477788\n",
      "test_runtime: 0.788\n",
      "test_samples_per_second: 38.07\n",
      "test_steps_per_second: 10.152\n",
      "\n",
      "---------------------- Training Model (Model:bio-clinical-bert, Lr:5e-05) ----------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at emilyalsentzer/Bio_ClinicalBERT and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 35/35 [00:09<00:00,  3.56it/s]\n",
      "100%|██████████| 35/35 [00:10<00:00,  3.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.4344639778137207, 'eval_overall_precision': 0.0, 'eval_overall_recall': 0.0, 'eval_overall_f1': 0.0, 'eval_overall_accuracy': 0.0, 'eval_runtime': 0.882, 'eval_samples_per_second': 34.014, 'eval_steps_per_second': 9.07, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35/35 [00:12<00:00,  2.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 12.002, 'train_samples_per_second': 11.665, 'train_steps_per_second': 2.916, 'train_loss': 2.7571629115513394, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:00<00:00, 11.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating best accuracy: None -> 0.0\n",
      "\n",
      "---------------------- Training Model (Model:bio-clinical-bert, Lr:4e-05) ----------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at emilyalsentzer/Bio_ClinicalBERT and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 35/35 [00:09<00:00,  3.56it/s]\n",
      "100%|██████████| 35/35 [00:10<00:00,  3.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.529207944869995, 'eval_overall_precision': 0.0, 'eval_overall_recall': 0.0, 'eval_overall_f1': 0.0, 'eval_overall_accuracy': 0.0, 'eval_runtime': 0.873, 'eval_samples_per_second': 34.364, 'eval_steps_per_second': 9.164, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35/35 [00:11<00:00,  2.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 11.947, 'train_samples_per_second': 11.718, 'train_steps_per_second': 2.93, 'train_loss': 2.8084527151925225, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:00<00:00, 11.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------------- Training Model (Model:bio-clinical-bert, Lr:3e-05) ----------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at emilyalsentzer/Bio_ClinicalBERT and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 35/35 [00:09<00:00,  3.58it/s]\n",
      "100%|██████████| 35/35 [00:10<00:00,  3.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.6124205589294434, 'eval_overall_precision': 0.0, 'eval_overall_recall': 0.0, 'eval_overall_f1': 0.0, 'eval_overall_accuracy': 0.0, 'eval_runtime': 0.8546, 'eval_samples_per_second': 35.106, 'eval_steps_per_second': 9.362, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35/35 [00:11<00:00,  2.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 11.8206, 'train_samples_per_second': 11.844, 'train_steps_per_second': 2.961, 'train_loss': 2.8759002685546875, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:00<00:00, 11.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------------- Training Model (Model:bio-clinical-bert, Lr:2e-05) ----------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at emilyalsentzer/Bio_ClinicalBERT and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 35/35 [00:09<00:00,  3.56it/s]\n",
      "100%|██████████| 35/35 [00:10<00:00,  3.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.703368663787842, 'eval_overall_precision': 0.0, 'eval_overall_recall': 0.0, 'eval_overall_f1': 0.0, 'eval_overall_accuracy': 0.0, 'eval_runtime': 0.8814, 'eval_samples_per_second': 34.039, 'eval_steps_per_second': 9.077, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35/35 [00:11<00:00,  2.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 11.8987, 'train_samples_per_second': 11.766, 'train_steps_per_second': 2.941, 'train_loss': 3.0064926147460938, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:00<00:00, 10.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving best bio-clinical-bert model...\n",
      "test_loss: 2.2728796005249023\n",
      "test_overall_precision: 0.0\n",
      "test_overall_recall: 0.0\n",
      "test_overall_f1: 0.0\n",
      "test_overall_accuracy: 0.0\n",
      "test_runtime: 0.782\n",
      "test_samples_per_second: 38.363\n",
      "test_steps_per_second: 10.23\n",
      "\n",
      "---------------------- Training Model (Model:medbert, Lr:5e-05) ----------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at Charangan/MedBERT and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 35/35 [00:09<00:00,  3.59it/s]\n",
      "100%|██████████| 35/35 [00:10<00:00,  3.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.2092010974884033, 'eval_overall_precision': 0.025787965616045846, 'eval_overall_recall': 0.016728624535315983, 'eval_overall_f1': 0.020293122886133032, 'eval_overall_accuracy': 0.13385655112273817, 'eval_runtime': 0.836, 'eval_samples_per_second': 35.885, 'eval_steps_per_second': 9.569, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35/35 [00:11<00:00,  3.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 11.411, 'train_samples_per_second': 12.269, 'train_steps_per_second': 3.067, 'train_loss': 2.652541242327009, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:00<00:00, 11.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating best accuracy: None -> 0.12050169968350721\n",
      "\n",
      "---------------------- Training Model (Model:medbert, Lr:4e-05) ----------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at Charangan/MedBERT and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 35/35 [00:09<00:00,  3.64it/s]\n",
      "100%|██████████| 35/35 [00:10<00:00,  3.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.2485191822052, 'eval_overall_precision': 0.03828972559029994, 'eval_overall_recall': 0.022304832713754646, 'eval_overall_f1': 0.028188865398167725, 'eval_overall_accuracy': 0.14333987355570088, 'eval_runtime': 0.8565, 'eval_samples_per_second': 35.026, 'eval_steps_per_second': 9.34, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35/35 [00:11<00:00,  2.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 11.7864, 'train_samples_per_second': 11.878, 'train_steps_per_second': 2.97, 'train_loss': 2.717863246372768, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:00<00:00, 11.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating best accuracy: 0.12050169968350721 -> 0.1315203375923104\n",
      "\n",
      "---------------------- Training Model (Model:medbert, Lr:3e-05) ----------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at Charangan/MedBERT and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 35/35 [00:09<00:00,  3.59it/s]\n",
      "100%|██████████| 35/35 [00:10<00:00,  3.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.443417549133301, 'eval_overall_precision': 0.0, 'eval_overall_recall': 0.0, 'eval_overall_f1': 0.0, 'eval_overall_accuracy': 0.038805319380858946, 'eval_runtime': 0.8545, 'eval_samples_per_second': 35.108, 'eval_steps_per_second': 9.362, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35/35 [00:11<00:00,  2.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 11.9504, 'train_samples_per_second': 11.715, 'train_steps_per_second': 2.929, 'train_loss': 2.9207567487444197, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:00<00:00, 11.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------------- Training Model (Model:medbert, Lr:2e-05) ----------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at Charangan/MedBERT and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 35/35 [00:09<00:00,  3.58it/s]\n",
      "100%|██████████| 35/35 [00:10<00:00,  3.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.6164839267730713, 'eval_overall_precision': 0.0, 'eval_overall_recall': 0.0, 'eval_overall_f1': 0.0, 'eval_overall_accuracy': 0.00021800741225201656, 'eval_runtime': 0.858, 'eval_samples_per_second': 34.965, 'eval_steps_per_second': 9.324, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35/35 [00:12<00:00,  2.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 12.0831, 'train_samples_per_second': 11.586, 'train_steps_per_second': 2.897, 'train_loss': 3.1084603445870536, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:00<00:00, 11.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving best medbert model...\n",
      "test_loss: 2.0795881748199463\n",
      "test_overall_precision: 0.058941728064300064\n",
      "test_overall_recall: 0.033846153846153845\n",
      "test_overall_f1: 0.043000244319569995\n",
      "test_overall_accuracy: 0.1315203375923104\n",
      "test_runtime: 0.801\n",
      "test_samples_per_second: 37.453\n",
      "test_steps_per_second: 9.988\n"
     ]
    }
   ],
   "source": [
    "evaluator = MRBNEREvaluator(metric=\"seqeval\", id2label=id2label)\n",
    "\n",
    "for m in MODELS:\n",
    "    save = {\n",
    "        \"save_trainer\": None,\n",
    "        \"test_metrics\": None,\n",
    "        \"param\": None\n",
    "    }\n",
    "    \n",
    "    for lr in LRATES: \n",
    "        print(f\"\\n---------------------- Training Model (Model:{m['name']}, Lr:{lr}) ----------------------\")\n",
    "\n",
    "        tokenizer = AutoTokenizer.from_pretrained(m[\"path\"])\n",
    "        data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "        model = AutoModelForTokenClassification.from_pretrained(\n",
    "            pretrained_model_name_or_path=m[\"path\"],\n",
    "            label2id=label2id,\n",
    "            id2label=id2label,\n",
    "            ignore_mismatched_sizes=True,\n",
    "            num_labels=len(label2id)\n",
    "        )\n",
    "\n",
    "        datasets = {}\n",
    "        datasets[\"train\"] = MRBNERDataset(data=data_splits[\"train\"], tokenizer=tokenizer, id2label=id2label, label2id=label2id, max_len=512)\n",
    "        datasets[\"val\"] = MRBNERDataset(data=data_splits[\"val\"], tokenizer=tokenizer, id2label=id2label, label2id=label2id, max_len=512)\n",
    "        datasets[\"test\"] = MRBNERDataset(data=data_splits[\"test\"], tokenizer=tokenizer, id2label=id2label, label2id=label2id, max_len=512)\n",
    "        \n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=f\"../output/{m['name']}/{lr}\",\n",
    "            overwrite_output_dir=True,\n",
    "            per_device_train_batch_size=BATCH_SIZE,\n",
    "            per_device_eval_batch_size=BATCH_SIZE,\n",
    "            num_train_epochs=EPOCHS,\n",
    "            learning_rate=lr,\n",
    "            weight_decay=WDECAY,\n",
    "            evaluation_strategy=\"epoch\",\n",
    "            save_strategy=\"epoch\",\n",
    "            load_best_model_at_end=True\n",
    "        )\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=datasets[\"train\"],\n",
    "            eval_dataset=datasets[\"val\"],\n",
    "            data_collator=data_collator,\n",
    "            tokenizer=tokenizer,\n",
    "            compute_metrics=evaluator.compute_metrics\n",
    "        )\n",
    "\n",
    "        trainer.train()\n",
    "        predictions, _, metrics = trainer.predict(datasets[\"test\"])\n",
    "\n",
    "        if save[\"test_metrics\"] == None or save[\"test_metrics\"][\"test_overall_accuracy\"] < metrics[\"test_overall_accuracy\"]:\n",
    "            best_accuracy = save['test_metrics']['test_overall_accuracy'] if save['test_metrics'] else None\n",
    "            print(f\"Updating best accuracy: {best_accuracy} -> {metrics['test_overall_accuracy']}\")\n",
    "\n",
    "            save[\"save_trainer\"] = trainer\n",
    "            save[\"test_metrics\"] = metrics\n",
    "            save[\"param\"] = lr\n",
    "\n",
    "    print(f\"\\nSaving best {m['name']} model...\")\n",
    "    for key, value in save[\"test_metrics\"].items():\n",
    "        print(f\"{key}: {value}\")\n",
    "\n",
    "    save[\"save_trainer\"].save_model(f\"../output/{m['name']}/save_model\")\n",
    "\n",
    "    del save, tokenizer, data_collator, model, datasets, training_args, trainer, predictions, _, metrics\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity_group': 'SIGN_SYMPTOM', 'score': 0.49153978, 'word': 'recurrence', 'start': 24, 'end': 34}, {'entity_group': 'SIGN_SYMPTOM', 'score': 0.702181, 'word': 'palpitations', 'start': 38, 'end': 50}, {'entity_group': 'DATE', 'score': 0.46011654, 'word': '6 months after', 'start': 64, 'end': 78}]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"tex2jax_ignore\" style=\"white-space: pre-wrap\">The patient reported no <span style=\"padding: 2px; border-radius: 4px; border: 1px solid #ffe0b2; background: #fff3e0\">recurrence<span style=\"vertical-align: middle; margin-left: 2px; font-size: 0.7em; color: #ffb74d;\">SIGN_SYMPTOM</span></span> of <span style=\"padding: 2px; border-radius: 4px; border: 1px solid #ffe0b2; background: #fff3e0\">palpitations<span style=\"vertical-align: middle; margin-left: 2px; font-size: 0.7em; color: #ffb74d;\">SIGN_SYMPTOM</span></span> at follow-up <span style=\"padding: 2px; border-radius: 4px; border: 1px solid #d1c4e9; background: #ede7f6\">6 months after<span style=\"vertical-align: middle; margin-left: 2px; font-size: 0.7em; color: #9575cd;\">DATE</span></span> the ablation.</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pipe = pipeline(\"ner\", model=model, tokenizer=tokenizer, device=0, aggregation_strategy=\"max\")  # device=0 (gpu)\n",
    "text = \"\"\"The patient reported no recurrence of palpitations at follow-up 6 months after the ablation.\"\"\"\n",
    "\n",
    "out = pipe(text)\n",
    "print(out)\n",
    "\n",
    "spans = []\n",
    "\n",
    "for row in out:\n",
    "    spans.append((row[\"start\"], row[\"end\"], row[\"entity_group\"]))\n",
    "\n",
    "show_span_box_markup(text, spans)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
